# Llama2-Chinese

## ğŸ—‚ï¸ å†…å®¹å¯¼å¼•
- [Llama2-Chinese](#llama2-chinese)
  - [ğŸ—‚ï¸ å†…å®¹å¯¼å¼•](#ï¸-å†…å®¹å¯¼å¼•)
  - [ğŸ¼ å›½å†…Llama2æœ€æ–°ä¸‹è½½åœ°å€ï¼](#-å›½å†…llama2æœ€æ–°ä¸‹è½½åœ°å€)
  - [ğŸ“¢ ç¤¾åŒºå…¬å‘Š](#-ç¤¾åŒºå…¬å‘Š)
  - [ğŸ“ æ•°æ®æ¥æº](#-æ•°æ®æ¥æº)
  - [â¬ æ¨¡å‹éƒ¨ç½²](#-æ¨¡å‹éƒ¨ç½²)
    - [æ¨¡å‹ä¸‹è½½](#æ¨¡å‹ä¸‹è½½)
      - [Metaå®˜æ–¹Llama2æ¨¡å‹](#metaå®˜æ–¹llama2æ¨¡å‹)
      - [åŸºäºLlama2çš„ä¸­æ–‡å¾®è°ƒæ¨¡å‹](#åŸºäºllama2çš„ä¸­æ–‡å¾®è°ƒæ¨¡å‹)
      - [åŸºäºLlama2çš„ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹Atom](#åŸºäºllama2çš„ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹atom)
    - [æ¨¡å‹è°ƒç”¨ä»£ç ç¤ºä¾‹](#æ¨¡å‹è°ƒç”¨ä»£ç ç¤ºä¾‹)
    - [FastAPIæ¥å£æ­å»º](#fastapiæ¥å£æ­å»º)
    - [Gradioå¿«é€Ÿæ­å»ºé—®ç­”å¹³å°](#gradioå¿«é€Ÿæ­å»ºé—®ç­”å¹³å°)
    - [Dockeréƒ¨ç½²é—®ç­”æ¥å£](#dockeréƒ¨ç½²é—®ç­”æ¥å£)
  - [ğŸ’¡ æ¨¡å‹å¾®è°ƒ](#-æ¨¡å‹å¾®è°ƒ)
    - [å¾®è°ƒè¿‡ç¨‹](#å¾®è°ƒè¿‡ç¨‹)
      - [Step1: ç¯å¢ƒå‡†å¤‡](#step1-ç¯å¢ƒå‡†å¤‡)
      - [Step2: æ•°æ®å‡†å¤‡](#step2-æ•°æ®å‡†å¤‡)
      - [Step3: å¾®è°ƒè„šæœ¬](#step3-å¾®è°ƒè„šæœ¬)
    - [åŠ è½½å¾®è°ƒæ¨¡å‹](#åŠ è½½å¾®è°ƒæ¨¡å‹)
  - [ğŸ„ æ¨¡å‹é‡åŒ–](#-æ¨¡å‹é‡åŒ–)
  - [ğŸš€ æ¨ç†åŠ é€Ÿ](#-æ¨ç†åŠ é€Ÿ)
    - [lmdeploy](#lmdeploy)
    - [FasterTransformer](#fastertransformer)
    - [vLLM](#vllm)
  - [ğŸ¥‡ æ¨¡å‹è¯„æµ‹](#-æ¨¡å‹è¯„æµ‹)
  - [ğŸ’ª å¤–å»¶èƒ½åŠ›](#-å¤–å»¶èƒ½åŠ›)
    - [LangChain](#langchain)
  - [ğŸ ä»£ç æ¨¡å‹](#-ä»£ç æ¨¡å‹)
  - [ğŸ“– å­¦ä¹ èµ„æ–™](#-å­¦ä¹ èµ„æ–™)
    - [Metaå®˜æ–¹å¯¹äºLlama2çš„ä»‹ç»](#metaå®˜æ–¹å¯¹äºllama2çš„ä»‹ç»)
    - [Llamaç›¸å…³è®ºæ–‡](#llamaç›¸å…³è®ºæ–‡)
    - [Llama2çš„è¯„æµ‹ç»“æœ](#llama2çš„è¯„æµ‹ç»“æœ)
  - [å‚è€ƒ](#å‚è€ƒ)

## ğŸ¼ å›½å†…Llama2æœ€æ–°ä¸‹è½½åœ°å€ï¼

æœ¬ä»“åº“ä¸­çš„ä»£ç ç¤ºä¾‹ä¸»è¦æ˜¯åŸºäºHugging Faceç‰ˆæœ¬å‚æ•°è¿›è¡Œè°ƒç”¨ï¼Œæˆ‘ä»¬æä¾›äº†è„šæœ¬å°†Metaå®˜ç½‘å‘å¸ƒçš„æ¨¡å‹å‚æ•°è½¬æ¢ä¸ºHugging Faceæ”¯æŒçš„æ ¼å¼ï¼Œå¯ä»¥ç›´æ¥é€šè¿‡transformersåº“è¿›è¡ŒåŠ è½½ï¼š[å‚æ•°æ ¼å¼è½¬åŒ–](https://github.com/FlagAlpha/Llama2-Chinese/blob/main/scripts/convert2hf/READMD.md)


<details>

- Llama2-7Bå®˜ç½‘ç‰ˆæœ¬ï¼šhttps://pan.xunlei.com/s/VN_kR2fwuJdG1F3CoF33rwpIA1?pwd=z9kf

- Llama2-7B-Chatå®˜ç½‘ç‰ˆæœ¬ï¼šhttps://pan.xunlei.com/s/VN_kQa1_HBvV-X9QVI6jV2kOA1?pwd=xmra

- Llama2-7B Hugging Faceç‰ˆæœ¬ï¼šhttps://pan.xunlei.com/s/VN_t0dUikZqOwt-5DZWHuMvqA1?pwd=66ep

- Llama2-7B-Chat Hugging Faceç‰ˆæœ¬ï¼šhttps://pan.xunlei.com/s/VN_oaV4BpKFgKLto4KgOhBcaA1?pwd=ufir

</details>

## ğŸ“¢ ç¤¾åŒºå…¬å‘Š

<details>

- 2023å¹´8æœˆ26æ—¥ï¼šæä¾›å°†MetaåŸå§‹æ¨¡å‹å‚æ•°è½¬æ¢ä¸ºå…¼å®¹Hugging Faceçš„[æ ¼å¼è½¬åŒ–è„šæœ¬](https://github.com/FlagAlpha/Llama2-Chinese/blob/main/scripts/convert2hf/READMD.md)ï¼

</details>

## ğŸ“ æ•°æ®æ¥æº

æˆ‘ä»¬é€šè¿‡ä»¥ä¸‹æ•°æ®æ¥ä¼˜åŒ–Llama2çš„ä¸­æ–‡èƒ½åŠ›:

| ç±»å‹                                                       | æè¿°                                                         |
| ---------------------------------------------------------- | ------------------------------------------------------------ |
| ç½‘ç»œæ•°æ®                                                   | äº’è”ç½‘ä¸Šå…¬å¼€çš„ç½‘ç»œæ•°æ®ï¼ŒæŒ‘é€‰å‡ºå»é‡åçš„é«˜è´¨é‡ä¸­æ–‡æ•°æ®ï¼Œæ¶‰åŠåˆ°ç™¾ç§‘ã€ä¹¦ç±ã€åšå®¢ã€æ–°é—»ã€å…¬å‘Šã€å°è¯´ç­‰é«˜è´¨é‡é•¿æ–‡æœ¬æ•°æ®ã€‚ |
| [Wikipedia](https://github.com/goldsmith/Wikipedia)        | ä¸­æ–‡Wikipediaçš„æ•°æ®                                          |
| [æ‚Ÿé“](https://github.com/BAAI-WuDao/Model)                | ä¸­æ–‡æ‚Ÿé“å¼€æºçš„200Gæ•°æ®                                       |
| [Clue](https://github.com/CLUEbenchmark/CLUEDatasetSearch) | Clueå¼€æ”¾çš„ä¸­æ–‡é¢„è®­ç»ƒæ•°æ®ï¼Œè¿›è¡Œæ¸…æ´—åçš„é«˜è´¨é‡ä¸­æ–‡é•¿æ–‡æœ¬æ•°æ®   |
| ç«èµ›æ•°æ®é›†                                                 | è¿‘å¹´æ¥ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†å¤šä»»åŠ¡ç«èµ›æ•°æ®é›†ï¼Œçº¦150ä¸ª              |
| [MNBVC](https://github.com/esbatmop/MNBVC)                 | MNBVC ä¸­æ¸…æ´—å‡ºæ¥çš„éƒ¨åˆ†æ•°æ®é›†                                 |

## â¬ æ¨¡å‹éƒ¨ç½²

Metaåœ¨ğŸ¤—Hugging Faceä¸Šæä¾›äº†æ‰€æœ‰æ¨¡å‹çš„ä¸‹è½½é“¾æ¥ï¼šhttps://huggingface.co/meta-llama

Llamaä¸­æ–‡ç¤¾åŒºçš„ä¸­æ–‡æ¨¡å‹ä¸‹è½½é“¾æ¥ï¼šhttps://huggingface.co/FlagAlpha

### æ¨¡å‹ä¸‹è½½

#### Metaå®˜æ–¹Llama2æ¨¡å‹

Llama2é¢„è®­ç»ƒæ¨¡å‹åŒ…å«7Bã€13Bå’Œ70Bä¸‰ä¸ªç‰ˆæœ¬ã€‚Llama2-Chatæ¨¡å‹åŸºäºé¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œäº†ç›‘ç£å¾®è°ƒï¼Œå…·å¤‡æ›´å¼ºçš„å¯¹è¯èƒ½åŠ›ã€‚

|  ç±»åˆ«  | æ¨¡å‹åç§°   | ğŸ¤—æ¨¡å‹åŠ è½½åç§°             | ä¸‹è½½åœ°å€                                                     |
|  ----------  | ---------- | ------------------------- | --------------------- |
|  é¢„è®­ç»ƒ  | Llama2-7B  | meta-llama/Llama-2-7b-hf  | [æ¨¡å‹ä¸‹è½½](https://huggingface.co/meta-llama/Llama-2-7b-hf)  |
|  é¢„è®­ç»ƒ  | Llama2-13B | meta-llama/Llama-2-13b-hf | [æ¨¡å‹ä¸‹è½½](https://huggingface.co/meta-llama/Llama-2-13b-hf) |
|  é¢„è®­ç»ƒ  | Llama2-70B | meta-llama/Llama-2-70b-hf | [æ¨¡å‹ä¸‹è½½](https://huggingface.co/meta-llama/Llama-2-70b-hf) |
|  Chat  | Llama2-7B-Chat  | meta-llama/Llama-2-7b-chat-hf  | [æ¨¡å‹ä¸‹è½½](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) |
|  Chat  | Llama2-13B-Chat | meta-llama/Llama-2-13b-chat-hf | [æ¨¡å‹ä¸‹è½½](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf) |
|  Chat  | Llama2-70B-Chat | meta-llama/Llama-2-70b-chat-hf | [æ¨¡å‹ä¸‹è½½](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf) |


#### åŸºäºLlama2çš„ä¸­æ–‡å¾®è°ƒæ¨¡å‹

æˆ‘ä»¬åŸºäºä¸­æ–‡æŒ‡ä»¤æ•°æ®é›†å¯¹Llama2-Chatæ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼Œä½¿å¾—Llama2æ¨¡å‹æœ‰ç€æ›´å¼ºçš„ä¸­æ–‡å¯¹è¯èƒ½åŠ›ã€‚LoRAå‚æ•°ä»¥åŠä¸åŸºç¡€æ¨¡å‹åˆå¹¶çš„å‚æ•°å‡å·²ä¸Šä¼ è‡³[Hugging Face](https://huggingface.co/FlagAlpha)ï¼Œç›®å‰åŒ…å«7Bå’Œ13Bçš„æ¨¡å‹ã€‚

|  ç±»åˆ«  | æ¨¡å‹åç§°   | ğŸ¤—æ¨¡å‹åŠ è½½åç§°             | åŸºç¡€æ¨¡å‹ç‰ˆæœ¬ |    ä¸‹è½½åœ°å€                                                     |
|  ----------  | ---------- | ------------- |  ----------------- | ------------------- |
|  åˆå¹¶å‚æ•° | Llama2-Chinese-7b-Chat | FlagAlpha/Llama2-Chinese-7b-Chat  |    meta-llama/Llama-2-7b-chat-hf       |[æ¨¡å‹ä¸‹è½½](https://huggingface.co/FlagAlpha/Llama2-Chinese-7b-Chat)  |
|  åˆå¹¶å‚æ•° | Llama2-Chinese-13b-Chat | FlagAlpha/Llama2-Chinese-13b-Chat|     meta-llama/Llama-2-13b-chat-hf     |[æ¨¡å‹ä¸‹è½½](https://huggingface.co/FlagAlpha/Llama2-Chinese-13b-Chat) |
|  LoRAå‚æ•° | Llama2-Chinese-7b-Chat-LoRA  | FlagAlpha/Llama2-Chinese-7b-Chat-LoRA  |     meta-llama/Llama-2-7b-chat-hf      |[æ¨¡å‹ä¸‹è½½](https://huggingface.co/FlagAlpha/Llama2-Chinese-7b-Chat-LoRA) |
|  LoRAå‚æ•° | Llama2-Chinese-13b-Chat-LoRA | FlagAlpha/Llama2-Chinese-13b-Chat-LoRA |     meta-llama/Llama-2-13b-chat-hf     |[æ¨¡å‹ä¸‹è½½](https://huggingface.co/FlagAlpha/Llama2-Chinese-13b-Chat-LoRA) |


#### åŸºäºLlama2çš„ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹Atom

ç¤¾åŒºæä¾›Atom-7Bæ¨¡å‹çš„å¼€æ”¾ä¸‹è½½ï¼Œæ¨¡å‹å‚æ•°ä¼šæŒç»­ä¸æ–­æ›´æ–°ï¼Œå…³äºæ¨¡å‹çš„è¿›å±•è¯¦è§ç¤¾åŒºå®˜ç½‘[llama.family](https://llama.family)ã€‚

| æ¨¡å‹åç§°        | ğŸ¤—æ¨¡å‹åŠ è½½åç§°                  | ä¸‹è½½åœ°å€                                                     |
| --------------- | ------------------------------ | ------------------------------------------------------------ |
| Atom-7B  | FlagAlpha/Atom-7B  | [æ¨¡å‹ä¸‹è½½](https://huggingface.co/FlagAlpha/Atom-7B) |


### æ¨¡å‹è°ƒç”¨ä»£ç ç¤ºä¾‹

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained('FlagAlpha/Atom-7B',device_map='auto',torch_dtype=torch.float16,load_in_8bit=True)
model =model.eval()
tokenizer = AutoTokenizer.from_pretrained('FlagAlpha/Atom-7B',use_fast=False)
tokenizer.pad_token = tokenizer.eos_token
input_ids = tokenizer(['<s>Human: ä»‹ç»ä¸€ä¸‹ä¸­å›½\n</s><s>Assistant: '], return_tensors="pt",add_special_tokens=False).input_ids.to('cuda')        
generate_input = {
    "input_ids":input_ids,
    "max_new_tokens":512,
    "do_sample":True,
    "top_k":50,
    "top_p":0.95,
    "temperature":0.3,
    "repetition_penalty":1.3,
    "eos_token_id":tokenizer.eos_token_id,
    "bos_token_id":tokenizer.bos_token_id,
    "pad_token_id":tokenizer.pad_token_id
}
generate_ids  = model.generate(**generate_input)
text = tokenizer.decode(generate_ids[0])
print(text)
```

### FastAPIæ¥å£æ­å»º

ä¸ºäº†æ–¹ä¾¿é€šè¿‡APIæ–¹å¼è°ƒç”¨æ¨¡å‹ï¼Œæˆ‘ä»¬æä¾›äº†è„šæœ¬ç”¨æ¥å¿«é€Ÿæ­å»º[FastAPI](https://github.com/tiangolo/fastapi)æ¥å£ï¼Œç›¸å…³æµ‹è¯•ä»£ç ä¸APIå‚æ•°è®¾ç½®è§[API è°ƒç”¨](https://github.com/FlagAlpha/Llama2-Chinese/blob/main/scripts/api/READMD.md)ã€‚

### Gradioå¿«é€Ÿæ­å»ºé—®ç­”å¹³å°

åŸºäºgradioæ­å»ºçš„é—®ç­”ç•Œé¢ï¼Œå®ç°äº†æµå¼çš„è¾“å‡ºï¼Œå°†ä¸‹é¢ä»£ç å¤åˆ¶åˆ°æ§åˆ¶å°è¿è¡Œï¼Œä»¥ä¸‹ä»£ç ä»¥Atom-7Bæ¨¡å‹ä¸ºä¾‹ï¼Œ<font color="#006600">ä¸åŒæ¨¡å‹åªéœ€ä¿®æ”¹ä¸€ä¸‹ä»£ç é‡Œçš„æ¨¡å‹åç§°å°±å¥½äº†ğŸ˜Š</font><br/>
```
python examples/chat_gradio.py --model_name_or_path FlagAlpha/Atom-7B
```

### Dockeréƒ¨ç½²é—®ç­”æ¥å£
è¯¦æƒ…å‚è§ï¼š[Dockeréƒ¨ç½²](https://github.com/FlagAlpha/Llama2-Chinese/blob/main/docs/chat_gradio_guide.md)

ç¬¬ä¸€æ­¥ï¼šå‡†å¤‡dockeré•œåƒï¼Œé€šè¿‡dockerå®¹å™¨å¯åŠ¨[chat_gradio.py](../examples/chat_gradio.py)
```bash
git clone https://github.com/FlagAlpha/Llama2-Chinese.git

cd Llama2-Chinese

docker build -f docker/Dockerfile -t flagalpha/llama2-chinese-7b:gradio .
```

ç¬¬äºŒæ­¥ï¼šé€šè¿‡docker-composeå¯åŠ¨chat_gradio
```bash
cd Llama2-Chinese/docker
doker-compose up -d --build
```


## ğŸ’¡ æ¨¡å‹å¾®è°ƒ

æœ¬ä»“åº“ä¸­æä¾›äº†åŸºäºLoRAçš„å¾®è°ƒä»£ç ï¼Œæœªæ¥æˆ‘ä»¬å°†ä¼šæ‰©å±•æ›´å¤šçš„å¾®è°ƒç®—æ³•ï¼Œæ•¬è¯·æœŸå¾…ï¼å…³äºLoRAçš„è¯¦ç»†ä»‹ç»å¯ä»¥å‚è€ƒè®ºæ–‡â€œ[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)â€ä»¥åŠå¾®è½¯Githubä»“åº“[LoRA](https://github.com/microsoft/LoRA)ã€‚

### å¾®è°ƒè¿‡ç¨‹

#### Step1: ç¯å¢ƒå‡†å¤‡

æ ¹æ®[requirements.txt](https://github.com/FlagAlpha/Llama2-Chinese/blob/main/requirements.txt)å®‰è£…å¯¹åº”çš„ç¯å¢ƒä¾èµ–ã€‚

#### Step2: æ•°æ®å‡†å¤‡
åœ¨dataç›®å½•ä¸‹æä¾›äº†ä¸€ä»½ç”¨äºæ¨¡å‹sftçš„æ•°æ®æ ·ä¾‹ï¼š
- è®­ç»ƒæ•°æ®ï¼š[data/train_sft.csv](https://github.com/FlagAlpha/Llama2-Chinese/blob/main/data/train_sft.csv)
- éªŒè¯æ•°æ®ï¼š[data/dev_sft.csv](https://github.com/FlagAlpha/Llama2-Chinese/blob/main/data/dev_sft.csv)

æ¯ä¸ªcsvæ–‡ä»¶ä¸­åŒ…å«ä¸€åˆ—â€œtextâ€ï¼Œæ¯ä¸€è¡Œä¸ºä¸€ä¸ªè®­ç»ƒæ ·ä¾‹ï¼Œæ¯ä¸ªè®­ç»ƒæ ·ä¾‹æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å°†é—®é¢˜å’Œç­”æ¡ˆç»„ç»‡ä¸ºæ¨¡å‹è¾“å…¥ï¼Œæ‚¨å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è‡ªå®šä¹‰è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ï¼š
```
"<s>Human: "+é—®é¢˜+"\n</s><s>Assistant: "+ç­”æ¡ˆ
```
ä¾‹å¦‚ï¼Œ
```
<s>Human: ç”¨ä¸€å¥è¯æè¿°åœ°çƒä¸ºä»€ä¹ˆæ˜¯ç‹¬ä¸€æ— äºŒçš„ã€‚</s><s>Assistant: å› ä¸ºåœ°çƒæ˜¯ç›®å‰ä¸ºæ­¢å”¯ä¸€å·²çŸ¥å­˜åœ¨ç”Ÿå‘½çš„è¡Œæ˜Ÿã€‚</s>
```

#### Step3: å¾®è°ƒè„šæœ¬

æˆ‘ä»¬æä¾›äº†ç”¨äºå¾®è°ƒçš„è„šæœ¬[train/sft/finetune.sh](https://github.com/FlagAlpha/Llama2-Chinese/blob/main/train/sft/finetune.sh)ï¼Œé€šè¿‡ä¿®æ”¹è„šæœ¬çš„éƒ¨åˆ†å‚æ•°å®ç°æ¨¡å‹çš„å¾®è°ƒï¼Œå…³äºå¾®è°ƒçš„å…·ä½“ä»£ç è§[train/sft/finetune_clm_lora.py](https://github.com/FlagAlpha/Llama2-Chinese/blob/main/train/sft/finetune_clm_lora.py)ï¼Œå•æœºå¤šå¡çš„å¾®è°ƒå¯ä»¥é€šè¿‡ä¿®æ”¹è„šæœ¬ä¸­çš„`--include localhost:0`æ¥å®ç°ã€‚


### åŠ è½½å¾®è°ƒæ¨¡å‹
å¾®è°ƒæ¨¡å‹å‚æ•°è§ï¼š[åŸºäºLlama2çš„ä¸­æ–‡å¾®è°ƒæ¨¡å‹](#åŸºäºllama2çš„ä¸­æ–‡å¾®è°ƒæ¨¡å‹)ï¼ŒLoRAå‚æ•°éœ€è¦å’ŒåŸºç¡€æ¨¡å‹å‚æ•°ç»“åˆä½¿ç”¨ã€‚

é€šè¿‡[PEFT](https://github.com/huggingface/peft)åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å‚æ•°å’Œå¾®è°ƒæ¨¡å‹å‚æ•°ï¼Œä»¥ä¸‹ç¤ºä¾‹ä»£ç ä¸­ï¼Œbase_model_name_or_pathä¸ºé¢„è®­ç»ƒæ¨¡å‹å‚æ•°ä¿å­˜è·¯å¾„ï¼Œfinetune_model_pathä¸ºå¾®è°ƒæ¨¡å‹å‚æ•°ä¿å­˜è·¯å¾„ã€‚

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel,PeftConfig
# ä¾‹å¦‚: finetune_model_path='FlagAlpha/Llama2-Chinese-7b-Chat-LoRA'
finetune_model_path=''  
config = PeftConfig.from_pretrained(finetune_model_path)
# ä¾‹å¦‚: base_model_name_or_path='meta-llama/Llama-2-7b-chat'
tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path,use_fast=False)
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,device_map='auto',torch_dtype=torch.float16,load_in_8bit=True)
model = PeftModel.from_pretrained(model, finetune_model_path, device_map={"": 0})
model =model.eval()
input_ids = tokenizer(['<s>Human: ä»‹ç»ä¸€ä¸‹åŒ—äº¬\n</s><s>Assistant: '], return_tensors="pt",add_special_tokens=False).input_ids.to('cuda')        
generate_input = {
    "input_ids":input_ids,
    "max_new_tokens":512,
    "do_sample":True,
    "top_k":50,
    "top_p":0.95,
    "temperature":0.3,
    "repetition_penalty":1.3,
    "eos_token_id":tokenizer.eos_token_id,
    "bos_token_id":tokenizer.bos_token_id,
    "pad_token_id":tokenizer.pad_token_id
}
generate_ids  = model.generate(**generate_input)
text = tokenizer.decode(generate_ids[0])
print(text)
```


<!-- ## ğŸš€ æœªæ¥è®¡åˆ’ -->


## ğŸ„ æ¨¡å‹é‡åŒ–
æˆ‘ä»¬å¯¹ä¸­æ–‡å¾®è°ƒçš„æ¨¡å‹å‚æ•°è¿›è¡Œäº†é‡åŒ–ï¼Œæ–¹ä¾¿ä»¥æ›´å°‘çš„è®¡ç®—èµ„æºè¿è¡Œã€‚ç›®å‰å·²ç»åœ¨[Hugging Face](https://huggingface.co/FlagAlpha)ä¸Šä¼ äº†13Bä¸­æ–‡å¾®è°ƒæ¨¡å‹[FlagAlpha/Llama2-Chinese-13b-Chat](https://huggingface.co/FlagAlpha/Llama2-Chinese-13b-Chat)çš„4bitå‹ç¼©ç‰ˆæœ¬[FlagAlpha/Llama2-Chinese-13b-Chat-4bit](https://huggingface.co/FlagAlpha/Llama2-Chinese-13b-Chat-4bit)ï¼Œå…·ä½“è°ƒç”¨æ–¹å¼å¦‚ä¸‹ï¼š
```python
from transformers import AutoTokenizer
from auto_gptq import AutoGPTQForCausalLM
model = AutoGPTQForCausalLM.from_quantized('FlagAlpha/Llama2-Chinese-13b-Chat-4bit', device="cuda:0")
tokenizer = AutoTokenizer.from_pretrained('FlagAlpha/Llama2-Chinese-13b-Chat-4bit',use_fast=False)
input_ids = tokenizer(['<s>Human: æ€ä¹ˆç™»ä¸Šç«æ˜Ÿ\n</s><s>Assistant: '], return_tensors="pt",add_special_tokens=False).input_ids.to('cuda')        
generate_input = {
    "input_ids":input_ids,
    "max_new_tokens":512,
    "do_sample":True,
    "top_k":50,
    "top_p":0.95,
    "temperature":0.3,
    "repetition_penalty":1.3,
    "eos_token_id":tokenizer.eos_token_id,
    "bos_token_id":tokenizer.bos_token_id,
    "pad_token_id":tokenizer.pad_token_id
}
generate_ids  = model.generate(**generate_input)
text = tokenizer.decode(generate_ids[0])
print(text)
```

## ğŸš€ æ¨ç†åŠ é€Ÿ
éšç€å¤§æ¨¡å‹å‚æ•°è§„æ¨¡çš„ä¸æ–­å¢é•¿ï¼Œåœ¨æœ‰é™çš„ç®—åŠ›èµ„æºä¸‹ï¼Œæå‡æ¨¡å‹çš„æ¨ç†é€Ÿåº¦é€æ¸å˜ä¸ºä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚å¸¸ç”¨çš„æ¨ç†åŠ é€Ÿæ¡†æ¶åŒ…å« lmdeployã€FasterTransformer å’Œ vLLM ç­‰ã€‚

### lmdeploy
[lmdeploy](https://github.com/InternLM/lmdeploy/) ç”±ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤å¼€å‘ï¼Œæ¨ç†ä½¿ç”¨ C++/CUDAï¼Œå¯¹å¤–æä¾› python/gRPC/http æ¥å£å’Œ WebUI ç•Œé¢ï¼Œæ”¯æŒ tensor parallel åˆ†å¸ƒå¼æ¨ç†ã€æ”¯æŒ fp16/weight int4/kv cache int8 é‡åŒ–ã€‚

è¯¦ç»†çš„æ¨ç†æ–‡æ¡£è§ï¼š[inference-speed/GPU/lmdeploy_example](https://github.com/FlagAlpha/Llama2-Chinese/tree/main/inference-speed/GPU/lmdeploy_example)

### FasterTransformer
[FasterTransformer](https://github.com/NVIDIA/FasterTransformer)ç”±NVIDIAå¼€å‘ï¼Œé‡‡ç”¨C++/CUDAç¼–å†™ï¼Œæ”¯æŒåˆ†å¸ƒå¼æ¨ç†ï¼Œtransformerç¼–ç å™¨å’Œè§£ç å™¨å‡å¯è¿›è¡ŒåŠ é€Ÿã€‚
é€šè¿‡FasterTransformerå’Œ[Triton](https://github.com/openai/triton)åŠ é€ŸLLama2æ¨¡å‹æ¨ç†ï¼Œç›®å‰æ”¯æŒFP16æˆ–è€…Int8æ¨ç†ï¼ŒInt4ç›®å‰è¿˜ä¸æ”¯æŒã€‚

è¯¦ç»†çš„æ¨ç†æ–‡æ¡£è§ï¼š[inference-speed/GPU/FasterTransformer_example](https://github.com/FlagAlpha/Llama2-Chinese/tree/main/inference-speed/GPU/FasterTransformer_example)

### vLLM
[vLLM](https://github.com/vllm-project/vllm)ç”±åŠ å·å¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡å¼€å‘ï¼Œæ ¸å¿ƒæŠ€æœ¯æ˜¯PageAttentionï¼Œååé‡æ¯”HuggingFace Transformersé«˜å‡º24å€ã€‚ç›¸è¾ƒä¸FasterTrainsformerï¼ŒvLLMæ›´åŠ çš„ç®€å•æ˜“ç”¨ï¼Œä¸éœ€è¦é¢å¤–è¿›è¡Œæ¨¡å‹çš„è½¬æ¢ï¼Œæ”¯æŒfp16æ¨ç†ã€‚

è¯¦ç»†çš„æ¨ç†æ–‡æ¡£è§ï¼š[inference-speed/GPU/vllm_example](https://github.com/FlagAlpha/Llama2-Chinese/blob/main/inference-speed/GPU/vllm_example/README.md)


## ğŸ¥‡ æ¨¡å‹è¯„æµ‹
ä¸ºäº†èƒ½å¤Ÿæ›´åŠ æ¸…æ™°åœ°äº†è§£Llama2æ¨¡å‹çš„ä¸­æ–‡é—®ç­”èƒ½åŠ›ï¼Œæˆ‘ä»¬ç­›é€‰äº†ä¸€äº›å…·æœ‰ä»£è¡¨æ€§çš„ä¸­æ–‡é—®é¢˜ï¼Œå¯¹Llama2æ¨¡å‹è¿›è¡Œæé—®ã€‚æˆ‘ä»¬æµ‹è¯•çš„æ¨¡å‹åŒ…å«Metaå…¬å¼€çš„Llama2-7B-Chatå’ŒLlama2-13B-Chatä¸¤ä¸ªç‰ˆæœ¬ï¼Œæ²¡æœ‰åšä»»ä½•å¾®è°ƒå’Œè®­ç»ƒã€‚æµ‹è¯•é—®é¢˜ç­›é€‰è‡ª[AtomBulb](https://github.com/AtomEcho/AtomBulb)ï¼Œå…±95ä¸ªæµ‹è¯•é—®é¢˜ï¼ŒåŒ…å«ï¼šé€šç”¨çŸ¥è¯†ã€è¯­è¨€ç†è§£ã€åˆ›ä½œèƒ½åŠ›ã€é€»è¾‘æ¨ç†ã€ä»£ç ç¼–ç¨‹ã€å·¥ä½œæŠ€èƒ½ã€ä½¿ç”¨å·¥å…·ã€äººæ ¼ç‰¹å¾å…«ä¸ªå¤§çš„ç±»åˆ«ã€‚

æµ‹è¯•ä¸­ä½¿ç”¨çš„Promptå¦‚ä¸‹ï¼Œä¾‹å¦‚å¯¹äºé—®é¢˜â€œåˆ—å‡º5ç§å¯ä»¥æ”¹å–„ç¡çœ è´¨é‡çš„æ–¹æ³•â€ï¼š
```
[INST] 
<<SYS>>
You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. The answer always been translate into Chinese language.

If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.

The answer always been translate into Chinese language.
<</SYS>>

åˆ—å‡º5ç§å¯ä»¥æ”¹å–„ç¡çœ è´¨é‡çš„æ–¹æ³•
[/INST]
```
Llama2-7B-Chatçš„æµ‹è¯•ç»“æœè§[meta_eval_7B.md](assets/meta_eval_7B.md)ï¼ŒLlama2-13B-Chatçš„æµ‹è¯•ç»“æœè§[meta_eval_13B.md](assets/meta_eval_13B.md)ã€‚

é€šè¿‡æµ‹è¯•æˆ‘ä»¬å‘ç°ï¼ŒMetaåŸå§‹çš„Llama2 Chatæ¨¡å‹å¯¹äºä¸­æ–‡é—®ç­”çš„å¯¹é½æ•ˆæœä¸€èˆ¬ï¼Œå¤§éƒ¨åˆ†æƒ…å†µä¸‹éƒ½ä¸èƒ½ç»™å‡ºä¸­æ–‡å›ç­”ï¼Œæˆ–è€…æ˜¯ä¸­è‹±æ–‡æ··æ‚çš„å½¢å¼ã€‚å› æ­¤ï¼ŒåŸºäºä¸­æ–‡æ•°æ®å¯¹Llama2æ¨¡å‹è¿›è¡Œè®­ç»ƒå’Œå¾®è°ƒååˆ†å¿…è¦ï¼Œæˆ‘ä»¬çš„ä¸­æ–‡ç‰ˆLlama2æ¨¡å‹ä¹Ÿå·²ç»åœ¨è®­ç»ƒä¸­ï¼Œè¿‘æœŸå°†å¯¹ç¤¾åŒºå¼€æ”¾ã€‚


## ğŸ’ª å¤–å»¶èƒ½åŠ›

é™¤äº†æŒç»­å¢å¼ºå¤§æ¨¡å‹å†…åœ¨çš„çŸ¥è¯†å‚¨å¤‡ã€é€šç”¨ç†è§£ã€é€»è¾‘æ¨ç†å’Œæƒ³è±¡èƒ½åŠ›ç­‰ï¼Œæœªæ¥ï¼Œæˆ‘ä»¬ä¹Ÿä¼šä¸æ–­ä¸°å¯Œå¤§æ¨¡å‹çš„å¤–å»¶èƒ½åŠ›ï¼Œä¾‹å¦‚çŸ¥è¯†åº“æ£€ç´¢ã€è®¡ç®—å·¥å…·ã€WolframAlphaã€æ“ä½œè½¯ä»¶ç­‰ã€‚
æˆ‘ä»¬é¦–å…ˆé›†æˆäº†LangChainæ¡†æ¶ï¼Œå¯ä»¥æ›´æ–¹ä¾¿åœ°åŸºäºLlama2å¼€å‘æ–‡æ¡£æ£€ç´¢ã€é—®ç­”æœºå™¨äººå’Œæ™ºèƒ½ä½“åº”ç”¨ç­‰ï¼Œå…³äºLangChainçš„æ›´å¤šä»‹ç»å‚è§[LangChain](https://github.com/langchain-ai/langchain)ã€‚
### LangChain
é’ˆå¯¹LangChainæ¡†æ¶å°è£…çš„Llama2 LLMç±»è§[examples/llama2_for_langchain.py](https://github.com/FlagAlpha/Llama2-Chinese/blob/main/examples/llama2_for_langchain.py)ï¼Œç®€å•çš„è°ƒç”¨ä»£ç ç¤ºä¾‹å¦‚ä¸‹ï¼š
```python
from llama2_for_langchain import Llama2

# è¿™é‡Œä»¥è°ƒç”¨4bité‡åŒ–å‹ç¼©çš„Llama2-Chineseå‚æ•°FlagAlpha/Llama2-Chinese-13b-Chat-4bitä¸ºä¾‹
llm = Llama2(model_name_or_path='FlagAlpha/Llama2-Chinese-13b-Chat-4bit', bit4=True)

while True:
    human_input = input("Human: ")
    response = llm(human_input)
    print(f"Llama2: {response}")
```

## ğŸ ä»£ç æ¨¡å‹
Metaå®˜æ–¹åœ¨2023å¹´8æœˆ24æ—¥å‘å¸ƒäº†å‘å¸ƒäº†Code Llamaï¼ŒåŸºäºä»£ç æ•°æ®å¯¹Llama2è¿›è¡Œäº†å¾®è°ƒï¼Œæä¾›ä¸‰ä¸ªä¸åŒåŠŸèƒ½çš„ç‰ˆæœ¬ï¼šåŸºç¡€æ¨¡å‹ï¼ˆCode Llamaï¼‰ã€Pythonä¸“ç”¨æ¨¡å‹ï¼ˆCode Llama - Pythonï¼‰å’ŒæŒ‡ä»¤è·Ÿéšæ¨¡å‹ï¼ˆCode Llama - Instructï¼‰ï¼ŒåŒ…å«7Bã€13Bã€34Bä¸‰ç§ä¸åŒå‚æ•°è§„æ¨¡ã€‚ä¸åŒæ¨¡å‹èƒ½åŠ›åŒºåˆ«å¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š

|  æ¨¡å‹ç±»åˆ«          |        æ¨¡å‹åç§°         | ä»£ç ç»­å†™ | ä»£ç å¡«å…… | æŒ‡ä»¤ç¼–ç¨‹ |
|-----------------------|------------------------|------|------|------|
| Code Llama            | CodeLlama-7b           | âœ…    | âœ…    | âŒ    |
|                       | CodeLlama-13b          | âœ…    | âœ…    | âŒ    |
|                       | CodeLlama-34b          | âœ…    | âŒ    | âŒ    |
| Code Llama - Python   | CodeLlama-7b-Python    | âœ…    | âŒ    | âŒ    |
|                       | CodeLlama-13b-Python   | âœ…    | âŒ    | âŒ    |
|                       | CodeLlama-34b-Python   | âœ…    | âŒ    | âŒ    |
| Code Llama - Instruct | CodeLlama-7b-Instruct  | âŒ    | âœ…    | âœ…    |
|                       | CodeLlama-13b-Instruct | âŒ    | âœ…    | âœ…    |
|                       | CodeLlama-34b-Instruct | âŒ    | âŒ    | âœ…    |

æˆ‘ä»¬æä¾›äº†Code Llamaçš„[å›½å†…ä¸‹è½½é“¾æ¥](#-å›½å†…llama2æœ€æ–°ä¸‹è½½åœ°å€ä¸Šçº¿)ä»¥åŠåœ¨çº¿ä½“éªŒåœ°å€[llama.family](https://llama.family/)ï¼Œå…³äºCode Llamaçš„è¯¦ç»†ä¿¡æ¯å¯ä»¥å‚è€ƒå®˜æ–¹Githubä»“åº“[codellama](https://github.com/facebookresearch/codellama)ã€‚


## ğŸ“– å­¦ä¹ èµ„æ–™
### Metaå®˜æ–¹å¯¹äº[Llama2](https://ai.meta.com/llama)çš„ä»‹ç»
è‡ªä»Metaå…¬å¸å‘å¸ƒç¬¬ä¸€ä»£LLaMAæ¨¡å‹ä»¥æ¥ï¼Œç¾Šé©¼æ¨¡å‹å®¶æ—ç¹è£å‘å±•ã€‚è¿‘æœŸMetaå‘å¸ƒäº†Llama2ç‰ˆæœ¬ï¼Œå¼€æºå¯å•†ç”¨ï¼Œåœ¨æ¨¡å‹å’Œæ•ˆæœä¸Šæœ‰äº†é‡å¤§æ›´æ–°ã€‚Llama2æ€»å…±å…¬å¸ƒäº†7Bã€13Bå’Œ70Bä¸‰ç§å‚æ•°å¤§å°çš„æ¨¡å‹ã€‚ç›¸æ¯”äºLLaMAï¼ŒLlama2çš„è®­ç»ƒæ•°æ®è¾¾åˆ°äº†2ä¸‡äº¿tokenï¼Œä¸Šä¸‹æ–‡é•¿åº¦ä¹Ÿç”±ä¹‹å‰çš„2048å‡çº§åˆ°4096ï¼Œå¯ä»¥ç†è§£å’Œç”Ÿæˆæ›´é•¿çš„æ–‡æœ¬ã€‚Llama2 Chatæ¨¡å‹åŸºäº100ä¸‡äººç±»æ ‡è®°æ•°æ®å¾®è°ƒå¾—åˆ°ï¼Œåœ¨è‹±æ–‡å¯¹è¯ä¸Šè¾¾åˆ°äº†æ¥è¿‘ChatGPTçš„æ•ˆæœã€‚      

### Llamaç›¸å…³è®ºæ–‡
* [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)
* [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)
* [Code Llama: Open Foundation Models for Code](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/)
### Llama2çš„è¯„æµ‹ç»“æœ
<p align="center" width="100%">
<img src="./assets/llama_eval.jpeg" style="width: 100%; display: block; margin: auto;">
</p>

## å‚è€ƒ

[1] Llama2-Chinese: [FlagAlpha/Llama2-Chinese](https://github.com/FlagAlpha/Llama2-Chinese), [Llama ä¸­æ–‡ç¤¾åŒº](https://llama.family/), [é£ä¹¦çŸ¥è¯†åº“æ–‡æ¡£](https://chinesellama.feishu.cn/wiki/space/7257824476874768388?ccm_open_type=lark_wiki_spaceLink)