# Llama2-Chinese

## ğŸ—‚ï¸ å†…å®¹å¯¼å¼•
- [Llama2-Chinese](#llama2-chinese)
  - [ğŸ—‚ï¸ å†…å®¹å¯¼å¼•](#ï¸-å†…å®¹å¯¼å¼•)
  - [å®‰è£…](#å®‰è£…)
  - [â¬ æ¨¡å‹éƒ¨ç½²](#-æ¨¡å‹éƒ¨ç½²)
    - [æ¨¡å‹ä¸‹è½½](#æ¨¡å‹ä¸‹è½½)
      - [åŸºäºLlama2çš„ä¸­æ–‡å¾®è°ƒæ¨¡å‹](#åŸºäºllama2çš„ä¸­æ–‡å¾®è°ƒæ¨¡å‹)
      - [åŸºäºLlama2çš„ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹Atom](#åŸºäºllama2çš„ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹atom)
    - [æ¨¡å‹è°ƒç”¨ä»£ç ç¤ºä¾‹](#æ¨¡å‹è°ƒç”¨ä»£ç ç¤ºä¾‹)
  - [ğŸ’¡ æ¨¡å‹å¾®è°ƒ](#-æ¨¡å‹å¾®è°ƒ)
    - [å¾®è°ƒè¿‡ç¨‹](#å¾®è°ƒè¿‡ç¨‹)
      - [Step1: ç¯å¢ƒå‡†å¤‡](#step1-ç¯å¢ƒå‡†å¤‡)
      - [Step2: æ•°æ®å‡†å¤‡](#step2-æ•°æ®å‡†å¤‡)
      - [Step3: å¾®è°ƒè„šæœ¬](#step3-å¾®è°ƒè„šæœ¬)
    - [åŠ è½½å¾®è°ƒæ¨¡å‹](#åŠ è½½å¾®è°ƒæ¨¡å‹)
  - [ğŸ’ª å¤–å»¶èƒ½åŠ›](#-å¤–å»¶èƒ½åŠ›)
    - [LangChain](#langchain)
  - [ğŸ“– å­¦ä¹ èµ„æ–™](#-å­¦ä¹ èµ„æ–™)
    - [Llamaç›¸å…³è®ºæ–‡](#llamaç›¸å…³è®ºæ–‡)
    - [Llama2çš„è¯„æµ‹ç»“æœ](#llama2çš„è¯„æµ‹ç»“æœ)
  - [å‚è€ƒ](#å‚è€ƒ)

## å®‰è£…

```shell
python -m venv env
source env/bin/activate
which python
pip install --upgrade pip
pip install torch transformers sentencepiece protobuf accelerate gradio bitsandbytes scipy -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
```

## â¬ æ¨¡å‹éƒ¨ç½²

### æ¨¡å‹ä¸‹è½½

#### åŸºäºLlama2çš„ä¸­æ–‡å¾®è°ƒæ¨¡å‹

|  ç±»åˆ«  | æ¨¡å‹åç§°   | ğŸ¤—æ¨¡å‹åŠ è½½åç§°             | åŸºç¡€æ¨¡å‹ç‰ˆæœ¬ |    ä¸‹è½½åœ°å€                                                     |
|  ----------  | ---------- | ------------- |  ----------------- | ------------------- |
|  åˆå¹¶å‚æ•° | Llama2-Chinese-7b-Chat | FlagAlpha/Llama2-Chinese-7b-Chat  |    meta-llama/Llama-2-7b-chat-hf       |[æ¨¡å‹ä¸‹è½½](https://huggingface.co/FlagAlpha/Llama2-Chinese-7b-Chat)  |
|  åˆå¹¶å‚æ•° | Llama2-Chinese-13b-Chat | FlagAlpha/Llama2-Chinese-13b-Chat|     meta-llama/Llama-2-13b-chat-hf     |[æ¨¡å‹ä¸‹è½½](https://huggingface.co/FlagAlpha/Llama2-Chinese-13b-Chat) |
|  LoRAå‚æ•° | Llama2-Chinese-7b-Chat-LoRA  | FlagAlpha/Llama2-Chinese-7b-Chat-LoRA  |     meta-llama/Llama-2-7b-chat-hf      |[æ¨¡å‹ä¸‹è½½](https://huggingface.co/FlagAlpha/Llama2-Chinese-7b-Chat-LoRA) |
|  LoRAå‚æ•° | Llama2-Chinese-13b-Chat-LoRA | FlagAlpha/Llama2-Chinese-13b-Chat-LoRA |     meta-llama/Llama-2-13b-chat-hf     |[æ¨¡å‹ä¸‹è½½](https://huggingface.co/FlagAlpha/Llama2-Chinese-13b-Chat-LoRA) |


#### åŸºäºLlama2çš„ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹Atom

| æ¨¡å‹åç§°        | ğŸ¤—æ¨¡å‹åŠ è½½åç§°                  | ä¸‹è½½åœ°å€                                                     |
| --------------- | ------------------------------ | ------------------------------------------------------------ |
| Atom-7B  | FlagAlpha/Atom-7B  | [æ¨¡å‹ä¸‹è½½](https://huggingface.co/FlagAlpha/Atom-7B) |


### æ¨¡å‹è°ƒç”¨ä»£ç ç¤ºä¾‹

1. [get_start.py](get_start.py)

## ğŸ’¡ æ¨¡å‹å¾®è°ƒ

### å¾®è°ƒè¿‡ç¨‹

#### Step1: ç¯å¢ƒå‡†å¤‡

æ ¹æ®[requirements.txt](https://github.com/FlagAlpha/Llama2-Chinese/blob/main/requirements.txt)å®‰è£…å¯¹åº”çš„ç¯å¢ƒä¾èµ–ã€‚

#### Step2: æ•°æ®å‡†å¤‡
åœ¨dataç›®å½•ä¸‹æä¾›äº†ä¸€ä»½ç”¨äºæ¨¡å‹sftçš„æ•°æ®æ ·ä¾‹ï¼š
- è®­ç»ƒæ•°æ®ï¼š[data/train_sft.csv](https://github.com/FlagAlpha/Llama2-Chinese/blob/main/data/train_sft.csv)
- éªŒè¯æ•°æ®ï¼š[data/dev_sft.csv](https://github.com/FlagAlpha/Llama2-Chinese/blob/main/data/dev_sft.csv)

æ¯ä¸ªcsvæ–‡ä»¶ä¸­åŒ…å«ä¸€åˆ—â€œtextâ€ï¼Œæ¯ä¸€è¡Œä¸ºä¸€ä¸ªè®­ç»ƒæ ·ä¾‹ï¼Œæ¯ä¸ªè®­ç»ƒæ ·ä¾‹æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å°†é—®é¢˜å’Œç­”æ¡ˆç»„ç»‡ä¸ºæ¨¡å‹è¾“å…¥ï¼Œæ‚¨å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è‡ªå®šä¹‰è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ï¼š
```
"<s>Human: "+é—®é¢˜+"\n</s><s>Assistant: "+ç­”æ¡ˆ
```
ä¾‹å¦‚ï¼Œ
```
<s>Human: ç”¨ä¸€å¥è¯æè¿°åœ°çƒä¸ºä»€ä¹ˆæ˜¯ç‹¬ä¸€æ— äºŒçš„ã€‚</s><s>Assistant: å› ä¸ºåœ°çƒæ˜¯ç›®å‰ä¸ºæ­¢å”¯ä¸€å·²çŸ¥å­˜åœ¨ç”Ÿå‘½çš„è¡Œæ˜Ÿã€‚</s>
```

#### Step3: å¾®è°ƒè„šæœ¬

æˆ‘ä»¬æä¾›äº†ç”¨äºå¾®è°ƒçš„è„šæœ¬[train/sft/finetune.sh](https://github.com/FlagAlpha/Llama2-Chinese/blob/main/train/sft/finetune.sh)ï¼Œé€šè¿‡ä¿®æ”¹è„šæœ¬çš„éƒ¨åˆ†å‚æ•°å®ç°æ¨¡å‹çš„å¾®è°ƒï¼Œå…³äºå¾®è°ƒçš„å…·ä½“ä»£ç è§[train/sft/finetune_clm_lora.py](https://github.com/FlagAlpha/Llama2-Chinese/blob/main/train/sft/finetune_clm_lora.py)ï¼Œå•æœºå¤šå¡çš„å¾®è°ƒå¯ä»¥é€šè¿‡ä¿®æ”¹è„šæœ¬ä¸­çš„`--include localhost:0`æ¥å®ç°ã€‚


### åŠ è½½å¾®è°ƒæ¨¡å‹
å¾®è°ƒæ¨¡å‹å‚æ•°è§ï¼š[åŸºäºLlama2çš„ä¸­æ–‡å¾®è°ƒæ¨¡å‹](#åŸºäºllama2çš„ä¸­æ–‡å¾®è°ƒæ¨¡å‹)ï¼ŒLoRAå‚æ•°éœ€è¦å’ŒåŸºç¡€æ¨¡å‹å‚æ•°ç»“åˆä½¿ç”¨ã€‚

é€šè¿‡[PEFT](https://github.com/huggingface/peft)åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å‚æ•°å’Œå¾®è°ƒæ¨¡å‹å‚æ•°ï¼Œä»¥ä¸‹ç¤ºä¾‹ä»£ç ä¸­ï¼Œbase_model_name_or_pathä¸ºé¢„è®­ç»ƒæ¨¡å‹å‚æ•°ä¿å­˜è·¯å¾„ï¼Œfinetune_model_pathä¸ºå¾®è°ƒæ¨¡å‹å‚æ•°ä¿å­˜è·¯å¾„ã€‚

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel,PeftConfig
# ä¾‹å¦‚: finetune_model_path='FlagAlpha/Llama2-Chinese-7b-Chat-LoRA'
finetune_model_path=''  
config = PeftConfig.from_pretrained(finetune_model_path)
# ä¾‹å¦‚: base_model_name_or_path='meta-llama/Llama-2-7b-chat'
tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path,use_fast=False)
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,device_map='auto',torch_dtype=torch.float16,load_in_8bit=True)
model = PeftModel.from_pretrained(model, finetune_model_path, device_map={"": 0})
model =model.eval()
input_ids = tokenizer(['<s>Human: ä»‹ç»ä¸€ä¸‹åŒ—äº¬\n</s><s>Assistant: '], return_tensors="pt",add_special_tokens=False).input_ids.to('cuda')        
generate_input = {
    "input_ids":input_ids,
    "max_new_tokens":512,
    "do_sample":True,
    "top_k":50,
    "top_p":0.95,
    "temperature":0.3,
    "repetition_penalty":1.3,
    "eos_token_id":tokenizer.eos_token_id,
    "bos_token_id":tokenizer.bos_token_id,
    "pad_token_id":tokenizer.pad_token_id
}
generate_ids  = model.generate(**generate_input)
text = tokenizer.decode(generate_ids[0])
print(text)
```

## ğŸ’ª å¤–å»¶èƒ½åŠ›

é™¤äº†æŒç»­å¢å¼ºå¤§æ¨¡å‹å†…åœ¨çš„çŸ¥è¯†å‚¨å¤‡ã€é€šç”¨ç†è§£ã€é€»è¾‘æ¨ç†å’Œæƒ³è±¡èƒ½åŠ›ç­‰ï¼Œæœªæ¥ï¼Œæˆ‘ä»¬ä¹Ÿä¼šä¸æ–­ä¸°å¯Œå¤§æ¨¡å‹çš„å¤–å»¶èƒ½åŠ›ï¼Œä¾‹å¦‚çŸ¥è¯†åº“æ£€ç´¢ã€è®¡ç®—å·¥å…·ã€WolframAlphaã€æ“ä½œè½¯ä»¶ç­‰ã€‚
æˆ‘ä»¬é¦–å…ˆé›†æˆäº†LangChainæ¡†æ¶ï¼Œå¯ä»¥æ›´æ–¹ä¾¿åœ°åŸºäºLlama2å¼€å‘æ–‡æ¡£æ£€ç´¢ã€é—®ç­”æœºå™¨äººå’Œæ™ºèƒ½ä½“åº”ç”¨ç­‰ï¼Œå…³äºLangChainçš„æ›´å¤šä»‹ç»å‚è§[LangChain](https://github.com/langchain-ai/langchain)ã€‚
### LangChain
é’ˆå¯¹LangChainæ¡†æ¶å°è£…çš„Llama2 LLMç±»è§[examples/llama2_for_langchain.py](https://github.com/FlagAlpha/Llama2-Chinese/blob/main/examples/llama2_for_langchain.py)ï¼Œç®€å•çš„è°ƒç”¨ä»£ç ç¤ºä¾‹å¦‚ä¸‹ï¼š
```python
from llama2_for_langchain import Llama2

# è¿™é‡Œä»¥è°ƒç”¨4bité‡åŒ–å‹ç¼©çš„Llama2-Chineseå‚æ•°FlagAlpha/Llama2-Chinese-13b-Chat-4bitä¸ºä¾‹
llm = Llama2(model_name_or_path='FlagAlpha/Llama2-Chinese-13b-Chat-4bit', bit4=True)

while True:
    human_input = input("Human: ")
    response = llm(human_input)
    print(f"Llama2: {response}")
```

## ğŸ“– å­¦ä¹ èµ„æ–™  

### Llamaç›¸å…³è®ºæ–‡
* [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)
* [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)
### Llama2çš„è¯„æµ‹ç»“æœ
<p align="center" width="100%">
<img src="./assets/llama_eval.jpeg" style="width: 100%; display: block; margin: auto;">
</p>

## å‚è€ƒ

[1] Llama2-Chinese: [FlagAlpha/Llama2-Chinese](https://github.com/FlagAlpha/Llama2-Chinese), [Llama ä¸­æ–‡ç¤¾åŒº](https://llama.family/), [é£ä¹¦çŸ¥è¯†åº“æ–‡æ¡£](https://chinesellama.feishu.cn/wiki/space/7257824476874768388?ccm_open_type=lark_wiki_spaceLink)

[2] [Wikipedia](https://github.com/goldsmith/Wikipedia)

[3] [æ‚Ÿé“](https://github.com/BAAI-WuDao/Model)

[4] [Clue](https://github.com/CLUEbenchmark/CLUEDatasetSearch)

[5] [MNBVC](https://github.com/esbatmop/MNBVC)

[6] [Errorï¼šAutoTokenizer.from_pretrainedï¼ŒUnboundLocalError: local variable 'sentencepiece_model_pb2' referenced before assignment](https://github.com/huggingface/transformers/issues/25848)

[7] [Loading Flan-T5 tokenizer throwing UnboundLocalError for variable sentencepiece_model_pb2](https://github.com/huggingface/transformers/issues/25667)

[8] [Offline mode](https://huggingface.co/docs/transformers/installation#offline-mode)